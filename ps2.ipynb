{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2: Linear Classifiers and Gradient Descent\n",
    "\n",
    "### Overview\n",
    "In this assignment, you will implement a simple linear classifier using both the SVM (hinge loss) and Softmax (cross-entropy loss) functions. You will also implement gradient descent to optimize the classifier. You will both complete this Python script (`ps2.ipynb`) and submit a LaTeX report (`ps2.tex` and `ps2.pdf`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Use the following code to generate a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spiral dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Score Function\n",
    "Implement a function to calculate the score function of a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Loss Functions\n",
    "Implement the following loss functions:\n",
    "  1. SVM Classifier (Hinge Loss).\n",
    "  2. Softmax Classifier (Cross-Entropy Loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Regularization\n",
    "Add L2 regularization to your loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Gradient Descent\n",
    "Implement gradient descent to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: LaTeX Write-Up\n",
    "Document your approach and results using LaTeX. You should include the following sections:\n",
    "  1. Introduction: Briefly describe the goal of the assignment.\n",
    "  2. Methodology:\n",
    "     - Explain the implementation of the score function.\n",
    "     - Describe the hinge loss and softmax loss.\n",
    "     - Discuss the regularization used.\n",
    "     - Explain the gradient descent method you implemented.\n",
    "  3. Results: Include any plots or results from your Python code.\n",
    "  4. Conclusion: Summarize your findings and any challenges you faced.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
